## Dimension reduction and feature representation

### Problem 1
Q: PCA is an exmaple of dimensional reduction method; give a full
derivation of PCA with the respect to its eigenvectors; explain SVD
and how it is used to solve PCA.

### Problem 2
Q: Compare regular PCA with the low-ranked PCA, what would be advantage
using the low-ranked PCA and how it is formulated?

### Problem 3
Q: For a low rank-regularized PCA, what would be the limie of dimension
reduction for a given p and n of your data?

### Problem 4
Q: What is the key motivation (and contribution) behind deep learning,
in terms of data representations?

### Problem 5
Q: What would be the true features of an object modeling problem? Why
does the feature decomposition in deep learning then a topological
recombination could make a better sampling? What would be the potential
problems making deep learning not a viable approach?

### Problem 6
Q: Explain the importance of appropriate feature selection being compatible
with model selection in the context model complexity.

### Problem 7
Q: What is the key motivation behind a kernel-based method in data
representaion?

### Problem 8
Q: What would be the ultimate and best representation for a high dimensional
and complex problem?

### Problem 9
Q: Give two examples to highlight the importance of selecting appropriate
dimensions for feature representations.

### Problem 10
Q: For a typical big data problem (p>>n), what considerations we will
have to take when trying to select an appropriate model (for instance,
to perform a SVM)?

